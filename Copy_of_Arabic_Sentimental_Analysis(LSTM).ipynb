{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Arabic Sentimental Analysis(LSTM).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbzSwECiYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.kaggle.com/mksaad/arabic-sentiment-analysis-in-tweets-nb-bow\n",
        "#https://github.com/saobou/arabic-text-preprocessing/blob/master/Preprocess.ipynb\n",
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk-dbusdCt3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk import NaiveBayesClassifier\n",
        "from nltk.metrics.scores import f_measure, precision, recall\n",
        "import collections\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rIa8Vw8ECIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_dataset = pd.read_csv('train_Arabic_tweets_positive_20190413.tsv',sep='\\t', encoding='utf-8')\n",
        "pos_test = pd.read_csv('test_Arabic_tweets_positive_20190413.tsv', sep='\\t', encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y69_zq_aFR2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neg_dataset = pd.read_csv('train_Arabic_tweets_negative_20190413.tsv',sep='\\t', encoding=\"utf-8\")\n",
        "neg_test = pd.read_csv('test_Arabic_tweets_negative_20190413.tsv',sep='\\t', encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcG6S_AwFgZo",
        "colab_type": "code",
        "outputId": "bc8c7a37-6500-4b18-e120-a72db064b546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "neg_dataset.head"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of       Sentiment                                               Text\n",
              "0           neg  اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...\n",
              "1           neg  توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...\n",
              "2           neg  #الاهلي_الهلال اكتب توقعك لنتيجة لقاء الهلال و...\n",
              "3           neg  نعمة المضادات الحيوية . تضع قطرة💧مضاد بنسلين ع...\n",
              "4           neg                             الدودو جايه تكمل علي 💔\n",
              "...         ...                                                ...\n",
              "22509       neg  كيف ترى أورانوس لو كان يقع مكان القمر ؟ 💙💙 كوك...\n",
              "22510       neg                                  احسدك على الايم 💔\n",
              "22511       neg                            لأول مرة ما بنكون سوا 💔\n",
              "22512       neg                                 بقله ليش يا واطي 🤔\n",
              "22513       neg  قد طال صبري في النوى إذ تركتني كئيبا ؛ غريبا ب...\n",
              "\n",
              "[22514 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfCR4MC4FjaZ",
        "colab_type": "code",
        "outputId": "6e49f15f-da1a-4930-83e6-fbefae7792b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pos_dataset['Text'][1]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال في روحك أماالمنبهرون بالمظا…'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIaIQNS5F2ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = pd.DataFrame(columns=['Text','Label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSpjIMxdF2fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neg = pd.DataFrame(columns=['Text','Label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgKp-sRBvxpn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "dec65d1b-efa9-43f6-f7a1-eef6cf318cbe"
      },
      "source": [
        "!pip install googletrans\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "translated = translator.translate(pos_dataset['Text'][1], src='ar', dest='en')\n",
        "print(translated.text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletrans) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (1.24.3)\n",
            "In the end, it will not stay Ahdala saw beauty in your soul Omaalmenbhron Balmaza ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXw4ItlVGgAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_tweets=[]\n",
        "neg_tweets=[]\n",
        "pos_sent=[]\n",
        "neg_sent=[]\n",
        "for i in range(len(pos_dataset)):\n",
        "  pos_tweets.append(pos_dataset['Text'][i])\n",
        "  pos_sent.append(1)\n",
        "\n",
        "for i in range(len(pos_test)):\n",
        "   pos_tweets.append(pos_test['Text'][i])\n",
        "   pos_sent.append(1)\n",
        "\n",
        "for i in range(len(neg_dataset)):\n",
        "  neg_tweets.append(neg_dataset['Text'][i])\n",
        "  neg_sent.append(0)\n",
        "\n",
        "for i in range(len(neg_test)):\n",
        "  neg_tweets.append(neg_test['Text'][i])\n",
        "  neg_sent.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x03rS217H_Z3",
        "colab_type": "code",
        "outputId": "13f8adff-705c-4451-d9ed-1d40b0094b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pos_tweets[1]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال في روحك أماالمنبهرون بالمظا…'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvn8wtyTICld",
        "colab_type": "code",
        "outputId": "950113c8-84be-4cea-f5ba-25ef094e58bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "neg_tweets[1]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'توقعت اذا جات داريا بشوفهم كاملين بس لي للحين احس فيه احد ناقصهم 💔 #Avlu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odYBtmiFKRTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=[]\n",
        "Label=[]\n",
        "for tweet in pos_tweets:\n",
        "  text.append(tweet)\n",
        "  Label.append(1)\n",
        "for tweet in neg_tweets:\n",
        "  text.append(tweet)\n",
        "  Label.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGvU-_GiKE3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.DataFrame(columns=['Text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg8Gl_KiIYEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['Text'] = text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQGApehOK8Dk",
        "colab_type": "code",
        "outputId": "d1821396-b602-4661-f1a9-a4c831362f0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>من الخير نفسه 💛</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56790</th>\n",
              "      <td>النوم وانت مكسور ده احساس غبي اللي هو مش قادر ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56791</th>\n",
              "      <td>استشهاد_الامام_كاظم_الغيظ السلام على المعذب في...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56792</th>\n",
              "      <td>انا كنت اكل الصحن بكبره 😐</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56793</th>\n",
              "      <td>قولوا لي ايش تشوفوا .. مع ملاحظة التلطف لأنه ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56794</th>\n",
              "      <td>✍ إذا أردت أن تعرف شيئا عني إسالني قبل أن تسأل...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>56795 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text\n",
              "0      نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...\n",
              "1      وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...\n",
              "2                                        من الخير نفسه 💛\n",
              "3      #زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...\n",
              "4      الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...\n",
              "...                                                  ...\n",
              "56790  النوم وانت مكسور ده احساس غبي اللي هو مش قادر ...\n",
              "56791  استشهاد_الامام_كاظم_الغيظ السلام على المعذب في...\n",
              "56792                          انا كنت اكل الصحن بكبره 😐\n",
              "56793  قولوا لي ايش تشوفوا .. مع ملاحظة التلطف لأنه ا...\n",
              "56794  ✍ إذا أردت أن تعرف شيئا عني إسالني قبل أن تسأل...\n",
              "\n",
              "[56795 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njmt6lfJAl03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "for sentence in data['Text']:\n",
        "  word_tokenize(sentence)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGj183NwAveV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e3fd935e-9cc6-4156-dda8-bb351fc06d06"
      },
      "source": [
        "data"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>من الخير نفسه 💛</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56790</th>\n",
              "      <td>النوم وانت مكسور ده احساس غبي اللي هو مش قادر ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56791</th>\n",
              "      <td>استشهاد_الامام_كاظم_الغيظ السلام على المعذب في...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56792</th>\n",
              "      <td>انا كنت اكل الصحن بكبره 😐</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56793</th>\n",
              "      <td>قولوا لي ايش تشوفوا .. مع ملاحظة التلطف لأنه ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56794</th>\n",
              "      <td>✍ إذا أردت أن تعرف شيئا عني إسالني قبل أن تسأل...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>56795 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text\n",
              "0      نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...\n",
              "1      وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...\n",
              "2                                        من الخير نفسه 💛\n",
              "3      #زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...\n",
              "4      الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...\n",
              "...                                                  ...\n",
              "56790  النوم وانت مكسور ده احساس غبي اللي هو مش قادر ...\n",
              "56791  استشهاد_الامام_كاظم_الغيظ السلام على المعذب في...\n",
              "56792                          انا كنت اكل الصحن بكبره 😐\n",
              "56793  قولوا لي ايش تشوفوا .. مع ملاحظة التلطف لأنه ا...\n",
              "56794  ✍ إذا أردت أن تعرف شيئا عني إسالني قبل أن تسأل...\n",
              "\n",
              "[56795 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTaMvClvLkbI",
        "colab_type": "text"
      },
      "source": [
        "### Arabic text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzww6PK5LTZr",
        "colab_type": "code",
        "outputId": "30eeaf5b-619b-4d2a-dfa0-e7c27470b8e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install Tashaphyne\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "from tashaphyne.stemming import ArabicLightStemmer\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "\n",
        "stops = set(stopwords.words(\"arabic\"))\n",
        "stop_word_comp = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\"}\n",
        "ArListem = ArabicLightStemmer()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Tashaphyne in /usr/local/lib/python3.6/dist-packages (0.3.4.1)\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.6/dist-packages (from Tashaphyne) (0.6.6)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vTdEUMOOnVl",
        "colab_type": "text"
      },
      "source": [
        "#### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMkfcf-lM0qQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def to_arabic(text):\n",
        "#     return ds.transliterate(text)\n",
        "\n",
        "def stem(text):\n",
        "    zen = TextBlob(text)\n",
        "    words = zen.words\n",
        "    cleaned = list()\n",
        "    for w in words:\n",
        "        ArListem.light_stem(w)\n",
        "        cleaned.append(ArListem.get_root())\n",
        "    return \" \".join(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsADo_l0OcN1",
        "colab_type": "text"
      },
      "source": [
        "#### Normalizing Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYbtjAScNalW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyarabic.araby as araby\n",
        "def normalizeArabic(text):\n",
        "    text = text.strip()\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(noise, '', text)\n",
        "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n",
        "    return araby.strip_tashkeel(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIAkhO3GOX-e",
        "colab_type": "text"
      },
      "source": [
        "#### Stop Words Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJZ35mgtNeOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stop_words(text):\n",
        "    zen = TextBlob(text)\n",
        "    words = zen.words\n",
        "    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw5D1ZIgNk_e",
        "colab_type": "text"
      },
      "source": [
        "### Dealing with Hashtags in string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1K3JcDqNhpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_hashtag_to_words(tag):\n",
        "    tag = tag.replace('#','')\n",
        "    tags = tag.split('_')\n",
        "    if len(tags) > 1 :\n",
        "        \n",
        "        return tags\n",
        "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
        "    return pattern.findall(tag)\n",
        "\n",
        "def clean_hashtag(text):\n",
        "    words = text.split()\n",
        "    text = list()\n",
        "    for word in words:\n",
        "        if is_hashtag(word):\n",
        "            text.extend(extract_hashtag(word))\n",
        "        else:\n",
        "            text.append(word)\n",
        "    return \" \".join(text)\n",
        "def is_hashtag(word):\n",
        "    if word.startswith(\"#\"):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "def extract_hashtag(text):\n",
        "    \n",
        "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
        "    word_list = []\n",
        "    for word in hash_list :\n",
        "        word_list.extend(split_hashtag_to_words(word))\n",
        "    return word_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkecUE8oSC2x",
        "colab_type": "text"
      },
      "source": [
        "### MISC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgcJmk0PN4bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def misc():\n",
        "  import re\n",
        "  import pyarabic.araby as araby\n",
        "\n",
        "  import warnings\n",
        "  warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "  # test sample\n",
        "  tweet1 = u'جهود تُبذل عامًا بعد عام تعبّر عن ترابط الجسد الواحد لخدمة ضيوف الرحمن؛ تطبيق ترجمان الفائز في #هاكاثون_الحج العام الماضي، ينطلق اليوم ليشارك الحجاج أيامهم الروحانية ويساعدهم في ترجمة اللوح الإرشادية في منطقة الحرم دون الحاجة إلى انترنت @reem. #حج_ذكي'\n",
        "\n",
        "\n",
        "\n",
        "  # removes usernames inside a tweet text caused by to mentions or reply tweets\n",
        "  def remove_usernames(input_txt, pattern):\n",
        "      r = re.findall(pattern, input_txt)\n",
        "      for i in r:\n",
        "          input_txt = re.sub(i, '', input_txt)\n",
        "\n",
        "      return input_txt\n",
        "\n",
        "  # removes arabic letters tashkeel like (ِ ًٍ ٌ  ّ ْ )\n",
        "  def remove_tashkeel(input_txt):\n",
        "      return araby.strip_tashkeel(input_txt)\n",
        "\n",
        "  # removes a decorative letter (ـ) which has no affect on words meaning\n",
        "  def remove_tatweel(input_txt):\n",
        "      return araby.strip_tatweel(input_txt)\n",
        "\n",
        "  # normalizes the different forms of the letter hamza (ئ ؤ) into a single form (ء)\n",
        "  def normalize_hamza(input_txt):\n",
        "      r = re.findall(u'ئ',input_txt);\n",
        "      e = re.findall(u'ؤ',input_txt);\n",
        "      for i in r:\n",
        "          input_txt= re.sub(i, u'ء', input_txt)\n",
        "      for i in e:\n",
        "          input_txt= re.sub(i, u'ء', input_txt)\n",
        "      return input_txt\n",
        "\n",
        "  # normalizes the different forms of the letter alef (آ أ إ) into a single form (ا)\n",
        "  def normalize_alef(input_txt):\n",
        "      r = re.findall(u'أ',input_txt);\n",
        "      e = re.findall(u'إ',input_txt);\n",
        "      o = re.findall(u'آ', input_txt);\n",
        "      for i in r:\n",
        "          input_txt= re.sub(i, u'ا', input_txt)\n",
        "      for i in e:\n",
        "          input_txt= re.sub(i, u'ا', input_txt)\n",
        "      for i in o:\n",
        "          input_txt= re.sub(i, u'ا', input_txt)\n",
        "      return input_txt\n",
        "\n",
        "  # normalizes the different forms of the letter yeh (ي ى) into a single form (ى)\n",
        "  def normalize_yeh(input_txt):\n",
        "      r = re.findall(u'ي', input_txt)\n",
        "      for i in r:\n",
        "          input_txt = re.sub(i, u'ى', input_txt)\n",
        "      return input_txt\n",
        "\n",
        "  # normalizes the different forms of the letter heh (ه ة) into a single form (ة)\n",
        "  def normalize_heh(input_txt):\n",
        "      r = re.findall(u'ه', input_txt)\n",
        "      for i in r:\n",
        "          input_txt = re.sub(i, u'ة', input_txt)\n",
        "      return input_txt\n",
        "\n",
        "  # combining all preprocessing functions together for testing samples\n",
        "  def full_preprocessing_steps(tweet):\n",
        "\n",
        "      print(\"Tweet before preprocessing: \" + tweet)\n",
        "\n",
        "      usernames_free = remove_usernames(tweet,  \"@[\\w]*\")\n",
        "      tashkeel_free = remove_tashkeel(usernames_free)\n",
        "      tatweel_free = remove_tatweel(tashkeel_free)\n",
        "      hamza_normalized = normalize_hamza(tatweel_free)\n",
        "      alef_normalized = normalize_alef(hamza_normalized)\n",
        "      yeh_normalized = normalize_yeh(alef_normalized)\n",
        "      heh_normalized = normalize_heh(yeh_normalized)\n",
        "\n",
        "      print(\"Tweet after preprocessing: \" + heh_normalized)\n",
        "\n",
        "  full_preprocessing_steps(tweet1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFgjChYPKuTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(text):\n",
        "    text = re.sub('#\\d+K\\d+', ' ', text)  # years like 2K19\n",
        "    text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
        "    text = re.sub('RT|cc', ' ', text)  # remove RT and cc\n",
        "    text = re.sub('@[^\\s]+',' ',text)\n",
        "    text = clean_hashtag(text)\n",
        "    # text = clean_emoji(text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05228C6v7Ck6",
        "colab_type": "text"
      },
      "source": [
        "#### DEMOJI\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nb4jbjH7BMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def demoji(sentence):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "  sentence = emoji_pattern.sub(r'', sentence)\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7S5LctfORM2",
        "colab_type": "text"
      },
      "source": [
        "#### Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQg0hOTiK0TS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    ## Clean for tweets\n",
        "    text = clean_tweet(text)\n",
        "    ## Remove punctuations\n",
        "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuation\n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', text)  \n",
        "    ## Remove Emojis\n",
        "    text = demoji(text)\n",
        "    ## Convert text to lowercases\n",
        "    text = text.lower()\n",
        "    ## Arabisy the text\n",
        "    # text = to_arabic(text)\n",
        "    ## Remove stop words\n",
        "    text = remove_stop_words(text)\n",
        "    ## Remove numbers\n",
        "    text = re.sub(\"\\d+\", \" \", text)\n",
        "    ## Remove Tashkeel\n",
        "    text = normalizeArabic(text)\n",
        "    #text = re.sub('\\W+', ' ', text)\n",
        "    text = re.sub('[A-Za-z]+',' ',text)\n",
        "    text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+',' ',text)\n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', text)  \n",
        "    #Stemming\n",
        "    #text = stem(text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SadxqiD7K-Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['Text'] = data['Text'].apply(lambda x:clean_text(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IIrIEaNMX5S",
        "colab_type": "code",
        "outputId": "b3b8ccbb-139f-4417-d9b8-8ef51c577e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data['Text'][1]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'النهايه يبقيٰ معك احدالا رايٰ الجمال روحك اماالمنبهرون بالمظا…'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZW1S0QWPHt-",
        "colab_type": "text"
      },
      "source": [
        "#### Common Words Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UxLcPjPOJRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freq = pd.Series(' '.join(data['Text']).split()).value_counts()[:12]\n",
        "freq = list(freq.index)\n",
        "data['Text'] = data['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu_9C7InPhzU",
        "colab_type": "text"
      },
      "source": [
        "#### Rare words removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl9UhDrmPe5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freq = pd.Series(' '.join(data['Text']).split()).value_counts()[-50:]\n",
        "freq = list(freq.index)\n",
        "data['Text'] = data['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXWWYdTG3-xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['Labels'] = Label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8MLJfXqPtIE",
        "colab_type": "code",
        "outputId": "126d845f-1b05-4b3d-8fc5-848c3e6b0d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>يتحول نود نقوله دعاء لله تبحثوا فينا اننا مكسو...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>النهايه يبقيٰ معك احدالا رايٰ الجمال روحك اماا...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>زلزل الملعب نصرنا بيلعب كن عالي الهمه ترضي بغي...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>الشيء الوحيد وصلوا للعالميه المسيار تري كانوا ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56790</th>\n",
              "      <td>النوم وانت مكسور ده احساس غبي مش قادر تنام لاز...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56791</th>\n",
              "      <td>استشهاد الامام كاظم الغيظ السلام المعذب قعر ال...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56792</th>\n",
              "      <td>كنت اكل الصحن بكبره</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56793</th>\n",
              "      <td>قولوا ايش تشوفوا ملاحظه التلطف لانه المود</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56794</th>\n",
              "      <td>اردت تعرف شيءا عني اسالني تسال غيري فعشاق التا...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>56795 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text  Labels\n",
              "0      يتحول نود نقوله دعاء لله تبحثوا فينا اننا مكسو...       1\n",
              "1      النهايه يبقيٰ معك احدالا رايٰ الجمال روحك اماا...       1\n",
              "2                                                              1\n",
              "3      زلزل الملعب نصرنا بيلعب كن عالي الهمه ترضي بغي...       1\n",
              "4      الشيء الوحيد وصلوا للعالميه المسيار تري كانوا ...       1\n",
              "...                                                  ...     ...\n",
              "56790  النوم وانت مكسور ده احساس غبي مش قادر تنام لاز...       0\n",
              "56791  استشهاد الامام كاظم الغيظ السلام المعذب قعر ال...       0\n",
              "56792                                كنت اكل الصحن بكبره       0\n",
              "56793          قولوا ايش تشوفوا ملاحظه التلطف لانه المود       0\n",
              "56794  اردت تعرف شيءا عني اسالني تسال غيري فعشاق التا...       0\n",
              "\n",
              "[56795 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqPGKvHYRL1z",
        "colab_type": "text"
      },
      "source": [
        "### Final Cleaning and Preparing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyBrzcIYQ83k",
        "colab_type": "code",
        "outputId": "c20ca906-2798-4835-98e0-9608235720f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# def avg_word(sentence):\n",
        "#     words = sentence.split()\n",
        "#     if len(words) == 0:\n",
        "#         return 0\n",
        "#     return (sum(len(word) for word in words)/len(words))\n",
        "\n",
        "# data['word_count'] = data['Text'].apply(lambda x: len(str(x).split(\" \")))\n",
        "# data['char_count'] = data['Text'].str.len() ## this also includes spaces\n",
        "# data['avg_char_per_word'] = data['Text'].apply(lambda x: avg_word(x))\n",
        "# stop = stopwords.words('arabic')\n",
        "# data['stopwords'] = data['Text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "# data = data.sort_values(by='word_count',ascending=[0])\n",
        "# final = []\n",
        "# for index, row in data.iterrows():\n",
        "#     if len(row['Text'].split()) > 3:\n",
        "#         final.append([row['Text'],row['Labels']])\n",
        "# df = pd.DataFrame(final)\n",
        "# df.columns = ['Text','Sentiment']\n",
        "# df.to_csv('final_data.csv',index=False)\n",
        "def deEmojify(data):\n",
        "  for sentence in data[\"Text\"]:\n",
        "    sentence.encode('ascii', 'ignore').decode('ascii')\n",
        "  return data\n",
        "\n",
        "deEmojify(data)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>يتحول نود نقوله دعاء لله تبحثوا فينا اننا مكسو...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>النهايه يبقيٰ معك احدالا رايٰ الجمال روحك اماا...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>زلزل الملعب نصرنا بيلعب كن عالي الهمه ترضي بغي...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>الشيء الوحيد وصلوا للعالميه المسيار تري كانوا ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56790</th>\n",
              "      <td>النوم وانت مكسور ده احساس غبي مش قادر تنام لاز...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56791</th>\n",
              "      <td>استشهاد الامام كاظم الغيظ السلام المعذب قعر ال...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56792</th>\n",
              "      <td>كنت اكل الصحن بكبره</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56793</th>\n",
              "      <td>قولوا ايش تشوفوا ملاحظه التلطف لانه المود</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56794</th>\n",
              "      <td>اردت تعرف شيءا عني اسالني تسال غيري فعشاق التا...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>56795 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text  Labels\n",
              "0      يتحول نود نقوله دعاء لله تبحثوا فينا اننا مكسو...       1\n",
              "1      النهايه يبقيٰ معك احدالا رايٰ الجمال روحك اماا...       1\n",
              "2                                                              1\n",
              "3      زلزل الملعب نصرنا بيلعب كن عالي الهمه ترضي بغي...       1\n",
              "4      الشيء الوحيد وصلوا للعالميه المسيار تري كانوا ...       1\n",
              "...                                                  ...     ...\n",
              "56790  النوم وانت مكسور ده احساس غبي مش قادر تنام لاز...       0\n",
              "56791  استشهاد الامام كاظم الغيظ السلام المعذب قعر ال...       0\n",
              "56792                                كنت اكل الصحن بكبره       0\n",
              "56793          قولوا ايش تشوفوا ملاحظه التلطف لانه المود       0\n",
              "56794  اردت تعرف شيءا عني اسالني تسال غيري فعشاق التا...       0\n",
              "\n",
              "[56795 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmsGmTzl31MY",
        "colab_type": "code",
        "outputId": "b2c7299a-7088-4876-d248-06e3b86b5066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "data = shuffle(data)\n",
        "data.reset_index()\n",
        "data.head"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                     Text  Labels\n",
              "15251  ˓٭˛ تطيب الخواطر وبامرك تتحقق الامنيات اننا اس...       1\n",
              "10311        الابتسامه رمز للفرح بان الدمعه رمز للحزن ف…       1\n",
              "11241  ليضاء قلبك صل محمد وال محمد اللہم صڵ ؏لء םבםנ ...       1\n",
              "43616                                      اتوقعها للنصر       0\n",
              "53446                                       امنيتنا كلنا       0\n",
              "...                                                  ...     ...\n",
              "20700                                       اذكار الصباح       1\n",
              "18964                                                بقي       1\n",
              "47115                عم صدق الجرس كاتبه شي منيح بنت نوال       0\n",
              "41520  جينيونق يلعب بهذا الشي الي بيده ابره للانف تنط...       0\n",
              "862                                            وانت بخير       1\n",
              "\n",
              "[56795 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f89q6bLbh4V1",
        "colab_type": "code",
        "outputId": "b2b2e3f7-7d8c-463d-d9ba-42f8b1e09ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "data.dropna()\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(data['Text'],data['Labels'], test_size = 0.10, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(51115,) (51115,)\n",
            "(5680,) (5680,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZJTHadbvWaL",
        "colab_type": "text"
      },
      "source": [
        "### Translated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24fMTLO3vVxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install googletrans\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmWc7d3Cwcxh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "15d6ed14-113d-4154-ac54-fb53b36b062d"
      },
      "source": [
        "d = ''' بلازما فيتامين لعلاج تساقط الشعر \n",
        "+ميزو لتغذية الشعر فقط800RS\n",
        "\n",
        "زراعة الاسنان الامريكية فقط3600RS\n",
        "للحجز 0125108000 '''\n",
        "translated = translator.translate(d, src='ar', dest='en')\n",
        "print(translated.text)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Plasma vitamin for hair loss treatment\n",
            "+ Mezzo to feed the hair only 800 RS\n",
            "\n",
            "American Dental Implant only 3600 RS\n",
            "For reservations 0125108000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmZglf2JmmEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras.preprocessing.text import Tokenizer\n",
        "MAX_NB_WORDS = 60000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZeGJ1ukmq6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS,lower=False)\n",
        "# tokenizer.fit_on_texts(np.concatenate((X_train, X_test)).ravel())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxV42ENjpEoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = pd.get_dummies(Y_train).values\n",
        "Y_test = pd.get_dummies(Y_test).values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-irAHZ2tpw4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZuDcD8Pmq1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# X_train_tokenized = pad_sequences(tokenizer.texts_to_sequences(X_train.values.ravel()),maxlen=MAX_SEQUENCE_LENGTH)\n",
        "# X_test_tokenized = pad_sequences(tokenizer.texts_to_sequences(X_test.values.ravel()),maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv95iEtJnNTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train_tokenized.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faRvCwgPnRC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train_tokenized[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atR_jON2ok7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn import decomposition\n",
        "# pca = decomposition.PCA(n_components=100)\n",
        "# pca.fit(X_train_tokenized)\n",
        "# X_train_tokenized = pca.transform(X_train_tokenized)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHpuPP5Ho2E1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_train[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9uPzwICEDet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import random as sparse_random\n",
        "from sklearn.random_projection import sparse_random_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tvec = TfidfVectorizer(max_features=MAX_SEQUENCE_LENGTH, encoding='utf-8',stop_words=[\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\"],ngram_range=(1, 3),max_df=0.85) #,\n",
        "X_train = tvec.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTpI5r-gNsx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBn4NSPKpBnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense, Dropout , GRU\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0nBuZ0BqQPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUrGlpYLqccB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJeDn5uUss8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = tvec.transform(X_test)\n",
        "accr = model.evaluate(X_test,Y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oal_mesIszj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdmq4tl0s-82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['acc'], label='train')\n",
        "plt.plot(history.history['val_acc'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grLPwgr9tFsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predictor(str):\n",
        "  new_tweet = [str]\n",
        "  seq = tvec.transform(new_tweet)\n",
        "  pred = model.predict(seq)\n",
        "  if(np.argmax(pred)) == 0:\n",
        "    return \"Negative\"\n",
        "  else:\n",
        "    return \"Positive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TOYxAErtard",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor('هذا مرهق. يلوم الديمقراطيون: روسيا تويتر موقع يوتيوب كومي جيل شتاين ويكيليكس Antifa ألفيين لكن الحزب لن ينظر إلى المرآة ويدرك أنه فشل تمامًا في تشكيل تحد ذي معنى للظلم الاجتماعي والاقتصادي  وبالتالي فقد الدعم.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2TggtMQuuSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmirmRUqvDh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}